{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myIC_oayo-JF",
        "colab_type": "text"
      },
      "source": [
        "Una LSTM (Long short-term memory) es un tipo de red neuronal recurrente (RNN) muy usada en problemas con dependencia temporal, que se compone de una unidad de memoria y tres reguladores que controlan el flujo de información a la unidad de memoria, \"input gate\", \"output gate\" y \"forget gate\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAoSoHFhplWk",
        "colab_type": "text"
      },
      "source": [
        "En PyTorch, las LSTM se implementan con la clase torch.nn.LSTM(*args, **kwargs).\n",
        "La clase tiene los siguientes parámetros, entre otros:\n",
        "*   Dimensión de la entrada.\n",
        "*   Dimensión del estado oculto h.\n",
        "*   Número de capas.\n",
        "\n",
        "La entrada tiene la siguiente forma (input, (h_0, c_0)):\n",
        "*   La entrada input es el tensor de la forma (longitud de la secuencia, número de instancias batch, dimensión de la entrada).\n",
        "*   h_0 es el estado oculto inicial.\n",
        "*   c_0 es el estado inicial de la unidad de memoria.\n",
        "\n",
        "La salida tiene la siguiente forma output, (h_n, c_n):\n",
        "*   La salida es el tensor de la forma (longitud de la secuencia, batch, num_directions * dimensión del estado oculto) con la salida para cada instante de la secuencia.\n",
        "*   Tensor h_n con la forma (num_capas * num_directions, batch, dimensión estado oculto) que contiene el estado oculto en el último instante.\n",
        "*   Tensor c_n con la forma (num_capas * num_directions, batch, dimensión estado oculto) que contiene el estado de la unidad de memoria en el último instante.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liEQc2AbooV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Creamos una instancia con una entrada de 10 dimensiones, una capa oculta de 6 dimeniones y 2 capas\n",
        "LSTM1 = nn.LSTM(10, 6, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEQVCjCCJvFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creamos la entrada, un batch de dos instancias, cada una con una secuencia de 5 time steps y dimensión 10\n",
        "input = torch.randn(5, 2, 10)\n",
        "\n",
        "#Inicializamos los estados iniciales para cada una de las instancias y cada una de las dos capas\n",
        "h0 = torch.randn(2, 2, 6)\n",
        "c0 = torch.randn(2, 2, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRhFAKVLKqWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "ff8e5f79-cf0f-4c9c-e726-96cc0c012a5d"
      },
      "source": [
        "#Llamamos al método forward de la instancia creada, pasándole la entrada y los estados iniciales creados\n",
        "output, (hn, cn) = LSTM1(input, (h0, c0))\n",
        "\n",
        "#La salida output tiene la forma (seq_len, batch, num_directions * hidden_size) y contiene la salida h_t de la última capa para todos los intantes de tiempo para cada instancia del batc.\n",
        "print(output)\n",
        "\n",
        "#La salida hn tiene la forma (num_layers * num_directions, batch, hidden_size) y contiene h_t para el último intantes de tiempo en cada capa.\n",
        "#Como se puede ver el último time step de la salida output coincide con la última capa de la salida hn\n",
        "print(hn)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0212,  0.0481,  0.0351,  0.3754, -0.0085, -0.1330],\n",
            "         [-0.2699,  0.2120, -0.2443, -0.0252,  0.2005,  0.1459]],\n",
            "\n",
            "        [[ 0.0352, -0.0233, -0.0744,  0.1915, -0.0332, -0.1680],\n",
            "         [-0.1555,  0.0230, -0.2110, -0.0331,  0.0401, -0.0915]],\n",
            "\n",
            "        [[ 0.0580, -0.0158, -0.1563,  0.0681, -0.0504, -0.1990],\n",
            "         [-0.0486,  0.0010, -0.2333, -0.0459, -0.0542, -0.1393]],\n",
            "\n",
            "        [[ 0.0535, -0.0114, -0.1836, -0.0287, -0.0979, -0.2382],\n",
            "         [ 0.0099, -0.0303, -0.2338, -0.0556, -0.0698, -0.1764]],\n",
            "\n",
            "        [[ 0.0896,  0.0014, -0.1426, -0.0814, -0.1498, -0.1807],\n",
            "         [ 0.0659, -0.0394, -0.1764, -0.0901, -0.1108, -0.1159]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "tensor([[[-0.2841, -0.0278,  0.1141, -0.1267,  0.3383,  0.0995],\n",
            "         [-0.0443,  0.1272,  0.1914,  0.2779,  0.0864, -0.0740]],\n",
            "\n",
            "        [[ 0.0896,  0.0014, -0.1426, -0.0814, -0.1498, -0.1807],\n",
            "         [ 0.0659, -0.0394, -0.1764, -0.0901, -0.1108, -0.1159]]],\n",
            "       grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
